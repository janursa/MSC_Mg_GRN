{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "680d82f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import sys\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "import graphviz\n",
    "from sklearn.model_selection import cross_val_score,cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cc5a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## build sklearn random forest\n",
    "n_estimators = 100\n",
    "criterion='squared_error'\n",
    "max_depth = None\n",
    "min_samples_split = 2\n",
    "min_samples_leaf = 1\n",
    "min_weight_fraction_leaf = 0\n",
    "max_features = sqrt(len(feature_names) - 1)/len(feature_names) # according to GENIE3 suggestion\n",
    "max_leaf_nodes = None\n",
    "min_impurity_decrease = 0\n",
    "bootstrap = True\n",
    "oob_score = True  # to use out-of-bag samples to estimate the generalization score (available for bootstrap=True)\n",
    "n_jobs = None # number of jobs in parallel. fit, predict, decision_path, and apply can be done in parallel over the trees\n",
    "random_state=None # controls randomness in bootstrapping as well as drawing features\n",
    "verbose=0 \n",
    "warm_start=False # reuse the slution of the previous call to fit and add more ensembles to the estimator. look up on Glosery\n",
    "ccp_alpha=0 # complexity parameter used for minima cost-complexity pruning. by default, no prunning\n",
    "max_samples = None # if bootstrap is True, the number of samples to draw from the samples. if none, draw X.shape[0]\n",
    "\n",
    "sk_rf_reg = RandomForestRegressor(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth, min_samples_split=min_samples_split,\n",
    "                                  min_samples_leaf=min_samples_leaf, min_weight_fraction_leaf=min_weight_fraction_leaf, \n",
    "                                  max_features=max_features, max_leaf_nodes=max_leaf_nodes, min_impurity_decrease=min_impurity_decrease,\n",
    "                                  bootstrap=bootstrap, oob_score=oob_score, n_jobs=n_jobs, random_state=random_state, verbose=verbose,\n",
    "                                  warm_start=warm_start, ccp_alpha=ccp_alpha, max_samples=max_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98d3575",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = ['neg_mean_squared_error','r2']\n",
    "return_estimator=True # returns the estimators for each k fold fit\n",
    "scores = cross_validate(sk_rf_reg,X_train,y_train,cv = 4,scoring=scoring,return_estimator=return_estimator) #\n",
    "print(scores.keys())\n",
    "estimators = scores['estimator']\n",
    "for estimator in estimators:\n",
    "    sk_rf_score = r2_score(estimator.predict(X_test),y_test)\n",
    "    print('rf scores test:',sk_rf_score)\n",
    "# print('mean score {} , std score {}'.format(scores.mean(),scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb21dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "est = HistGradientBoostingRegressor().fit(X, y)\n",
    "est.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6924e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, ensemble\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "diabetes = datasets.load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=13\n",
    ")\n",
    "\n",
    "params = {\n",
    "    \"n_estimators\": 500,\n",
    "    \"max_depth\": 4,\n",
    "    \"min_samples_split\": 5,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"loss\": \"squared_error\",\n",
    "}\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "reg = ensemble.GradientBoostingRegressor(**params)\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "mse = r2_score(y_test, reg.predict(X_test))\n",
    "print(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse))\n",
    "test_score = np.zeros((params[\"n_estimators\"],), dtype=np.float64)\n",
    "for i, y_pred in enumerate(reg.staged_predict(X_test)):\n",
    "    test_score[i] = reg.loss_(y_test, y_pred)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.title(\"Deviance\")\n",
    "plt.plot(\n",
    "    np.arange(params[\"n_estimators\"]) + 1,\n",
    "    reg.train_score_,\n",
    "    \"b-\",\n",
    "    label=\"Training Set Deviance\",\n",
    ")\n",
    "plt.plot(\n",
    "    np.arange(params[\"n_estimators\"]) + 1, test_score, \"r-\", label=\"Test Set Deviance\"\n",
    ")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"Boosting Iterations\")\n",
    "plt.ylabel(\"Deviance\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "## feature importance\n",
    "feature_importance = reg.feature_importances_\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + 0.5\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(pos, feature_importance[sorted_idx], align=\"center\")\n",
    "plt.yticks(pos, np.array(diabetes.feature_names)[sorted_idx])\n",
    "plt.title(\"Feature Importance (MDI)\")\n",
    "\n",
    "result = permutation_importance(\n",
    "    reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n",
    ")\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(\n",
    "    result.importances[sorted_idx].T,\n",
    "    vert=False,\n",
    "    labels=np.array(diabetes.feature_names)[sorted_idx],\n",
    ")\n",
    "plt.title(\"Permutation Importance (test set)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e446ba1",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'oob_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 24>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m model_selection\u001b[38;5;241m.\u001b[39mtrain_test_split(X,y,test_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m     16\u001b[0m params_train \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#     \"max_iter\": 100,\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#     \"max_depth\": 10,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moob_score\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     23\u001b[0m }\n\u001b[1;32m---> 24\u001b[0m reg \u001b[38;5;241m=\u001b[39m \u001b[43mensemble\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGradientBoostingRegressor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m est \u001b[38;5;241m=\u001b[39m reg\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'oob_score'"
     ]
    }
   ],
   "source": [
    "## histGradientBoostingReg example\n",
    "import sys\n",
    "main_dir = \"C:/Users/nourisa/Downloads/testProjs/MSC_Mg_omics\"\n",
    "sys.path.insert(0,main_dir)\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn import model_selection\n",
    "from sklearn import ensemble\n",
    "import utils as fn\n",
    "import importlib\n",
    "importlib.reload(fn)\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "X, y,features_names = diabetes['data'],diabetes['target'],diabetes['feature_names']\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X,y,test_size = 0.1)\n",
    "params_train = {\n",
    "#     \"max_iter\": 100,\n",
    "#     \"max_depth\": 10,\n",
    "#     \"learning_rate\": 0.05,\n",
    "#     \"loss\": \"squared_error\",\n",
    "#     \"l2_regularization\": None,\n",
    "    'oob_score': True\n",
    "}\n",
    "reg = ensemble.GradientBoostingRegressor(**params_train)\n",
    "est = reg.fit(X_train, y_train)\n",
    "# params_FI = {\n",
    "#     'n_repeats': 30,\n",
    "#     'random_state': 0, \n",
    "#     'n_jobs': 1, \n",
    "#     'scoring': 'r2'\n",
    "# }\n",
    "# result,fig = fn.feature_importance(reg,X_test, y_test,features_names, params_FI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e2bfde0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.408557872691686"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a19734ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp n: 4\n",
      "time*genes (15, 506)\n",
      "len of alphas: 506\n",
      "len of genes: 506\n",
      "len of TFs: 29\n"
     ]
    }
   ],
   "source": [
    "## load the data\n",
    "import sys\n",
    "import os\n",
    "main_dir = \"C:/Users/nourisa/Downloads/testProjs/omics\"\n",
    "sys.path.insert(0,main_dir)\n",
    "import _pickle\n",
    "\n",
    "data_dir = 'C:/Users/nourisa/Downloads/testProjs/omics/dynGENIE3/dynGENIE3_data/real_networks/data/yeast_data.pkl'\n",
    "goldern_links_dir = ''\n",
    "with open(data_dir,'rb') as f:\n",
    "    (TS_data, time_points, genes, TFs, alphas) = _pickle.load(f)\n",
    "print('exp n:',len(TS_data))\n",
    "print('time*genes',TS_data[0].shape)\n",
    "print('len of alphas:',len(alphas))\n",
    "print('len of genes:',len(genes))\n",
    "print('len of TFs:',len(TFs))\n",
    "# print(genes)\n",
    "# Xs,ys = newEstimator.process_data(TS_data, time_points)\n",
    "# print('genes: ',len(Xs))\n",
    "# print('time across all experiments*genes:',Xs[0].shape)\n",
    "# check_estimator(TargetEstimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5d2f8fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Xs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m param \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m      8\u001b[0m     regressor_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRF\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      9\u001b[0m     n_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m     10\u001b[0m     n_estimators \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m     11\u001b[0m     max_depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     13\u001b[0m est \u001b[38;5;241m=\u001b[39m newEstimator\u001b[38;5;241m.\u001b[39mTargetEstimator(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparam)\n\u001b[1;32m---> 14\u001b[0m est\u001b[38;5;241m.\u001b[39mfit(\u001b[43mXs\u001b[49m[i_gene],ys[i_gene])\n\u001b[0;32m     15\u001b[0m est\u001b[38;5;241m.\u001b[39mscore(Xs[i_gene],ys[i_gene])\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# preds = est.predict(Xs[i_gene])\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Xs' is not defined"
     ]
    }
   ],
   "source": [
    "## single target estimator\n",
    "from dynGENIE3 import newEstimator \n",
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "import importlib\n",
    "importlib.reload(newEstimator)\n",
    "i_gene = 0\n",
    "param = dict(\n",
    "    regressor_t = 'RF',\n",
    "    n_jobs = 4,\n",
    "    n_estimators = 100,\n",
    "    max_depth = 20,\n",
    ")\n",
    "est = newEstimator.TargetEstimator(**param)\n",
    "est.fit(Xs[i_gene],ys[i_gene])\n",
    "est.score(Xs[i_gene],ys[i_gene])\n",
    "# preds = est.predict(Xs[i_gene])\n",
    "est.weight_links()\n",
    "# check_estimator(newEstimator.TargetEstimator(**param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dd15bdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## grid search\n",
    "from dynGENIE3 import newEstimator \n",
    "import importlib\n",
    "import numpy as np\n",
    "importlib.reload(newEstimator)\n",
    "param = dict(\n",
    "    regressor_t = 'RF', \n",
    "    n_estimators = 100,\n",
    ")\n",
    "param_grid = dict(\n",
    "    max_depth = [10,20,30]\n",
    ")\n",
    "specs = dict(\n",
    "    n_jobs = 4,\n",
    "    cv = 5,\n",
    "    error_score = True\n",
    ")\n",
    "best_scores, best_params, best_fitted_est = newEstimator.grid_search(Xs,ys, param, param_grid, specs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6b345b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "## running network inference with best estimators\n",
    "from dynGENIE3 import newEstimator \n",
    "import importlib\n",
    "import numpy as np\n",
    "importlib.reload(newEstimator)\n",
    "ests, scores_train, links = newEstimator.network_inference(Xs,ys, ests = best_fitted_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0ff3fab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## running network inference with best parameters\n",
    "from dynGENIE3 import newEstimator \n",
    "import importlib\n",
    "import numpy as np\n",
    "importlib.reload(newEstimator)\n",
    "param = dict(\n",
    "    regressor_t = 'RF', \n",
    "    n_estimators = 100,\n",
    ")\n",
    "ests, scores_train, links = newEstimator.network_inference(Xs,ys,param=param,param_unique =best_params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a44ae59",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "spec not found for the module 'dynGENIE3.newEstimator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnewEstimator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m links_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/links.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      7\u001b[0m newEstimator\u001b[38;5;241m.\u001b[39mget_link_list(links,file_name\u001b[38;5;241m=\u001b[39mlinks_dir)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\py38\\lib\\importlib\\__init__.py:168\u001b[0m, in \u001b[0;36mreload\u001b[1;34m(module)\u001b[0m\n\u001b[0;32m    166\u001b[0m spec \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m__spec__ \u001b[38;5;241m=\u001b[39m _bootstrap\u001b[38;5;241m.\u001b[39m_find_spec(name, pkgpath, target)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 168\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspec not found for the module \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m    169\u001b[0m _bootstrap\u001b[38;5;241m.\u001b[39m_exec(spec, module)\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The module may have replaced itself in sys.modules!\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: spec not found for the module 'dynGENIE3.newEstimator'"
     ]
    }
   ],
   "source": [
    "## process the links\n",
    "from dynGENIE3 import newEstimator \n",
    "import importlib\n",
    "import numpy as np\n",
    "importlib.reload(newEstimator)\n",
    "links_dir = 'results/links.txt'\n",
    "newEstimator.get_link_list(links,file_name=links_dir)\n",
    "with open(links_dir,'r') as f:\n",
    "    raw_ = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9ae6039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ST   ST_conf\n",
      "x1  0.555860  0.081271\n",
      "x2  0.441898  0.039816\n",
      "x3  0.244675  0.029211\n",
      "          S1   S1_conf\n",
      "x1  0.316832  0.065139\n",
      "x2  0.443763  0.049153\n",
      "x3  0.012203  0.055955\n",
      "                S2   S2_conf\n",
      "(x1, x2)  0.009254  0.074836\n",
      "(x1, x3)  0.238172  0.099015\n",
      "(x2, x3) -0.004888  0.058878\n",
      "[0.31683154 0.44376306 0.01220312]\n"
     ]
    }
   ],
   "source": [
    "from SALib.sample import saltelli\n",
    "from SALib.analyze import sobol\n",
    "from SALib.test_functions import Ishigami\n",
    "import numpy as np\n",
    "\n",
    "# Define the model inputs\n",
    "problem = {\n",
    "    'num_vars': 3,\n",
    "    'names': ['x1', 'x2', 'x3'],\n",
    "    'bounds': [[-3.14159265359, 3.14159265359],\n",
    "               [-3.14159265359, 3.14159265359],\n",
    "               [-3.14159265359, 3.14159265359]]\n",
    "}\n",
    "\n",
    "# Generate samples\n",
    "param_values = saltelli.sample(problem, 1024)\n",
    "\n",
    "# Run model (example)\n",
    "Y = Ishigami.evaluate(param_values)\n",
    "\n",
    "# Perform analysis\n",
    "Si = sobol.analyze(problem, Y, print_to_console=True)\n",
    "\n",
    "# Print the first-order sensitivity indices\n",
    "print(Si['S1'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
